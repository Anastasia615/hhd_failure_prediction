{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e13fd4-e79e-402b-bd0d-8350facf3bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 17:41:07,412 INFO: Сортировка данных...\n",
      "2024-12-08 17:41:08,332 INFO: Формирование целевой метки (failure_future_30d)...\n",
      "Обработка дисков: 100%|███████████████████| 10927/10927 [02:15<00:00, 80.53it/s]\n",
      "2024-12-08 17:43:25,258 INFO: Удаление записей без будущего периода 30 дней...\n",
      "2024-12-08 17:43:29,040 INFO: Генерация признаков...\n",
      "Генерация признаков: 100%|███████████████| 10912/10912 [00:46<00:00, 233.91it/s]\n",
      "2024-12-08 17:44:24,190 INFO: Подготовка данных для обучения...\n",
      "2024-12-08 17:44:24,690 INFO: Разделение на train/test по времени...\n",
      "2024-12-08 17:44:26,854 INFO: Первичное обучение для оценки важности признаков...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 118, number of negative: 5287919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.136642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4983\n",
      "[LightGBM] [Info] Number of data points in the train set: 5288037, number of used features: 45\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000022 -> initscore=-10.710251\n",
      "[LightGBM] [Info] Start training from score -10.710251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 17:44:39,639 INFO: Дисбаланс классов: 5287919 отриц. vs 118 полож., scale_pos_weight=44812.872881355936\n",
      "2024-12-08 17:44:39,640 INFO: Применение SMOTE для балансировки...\n",
      "2024-12-08 17:44:46,232 INFO: Обучение финальной модели LightGBM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5287919, number of negative: 5287919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7650\n",
      "[LightGBM] [Info] Number of data points in the train set: 10575838, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 17:45:55,356 INFO: Предсказание вероятностей на тестовой выборке...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "\n",
    "#################################\n",
    "# 1. Загрузка данных\n",
    "#################################\n",
    "# Предполагается, что df уже загружен\n",
    "# Пример:\n",
    "df = pd.read_csv(\"ST14000NM001G.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Столбцы должны включать 'date', 'serial_number', 'failure', 'capacity_bytes' и SMART-атрибуты.\n",
    "# Убедитесь, что столбцы типа 'smart_5_raw', 'smart_9_raw' и т.д. присутствуют.\n",
    "\n",
    "#################################\n",
    "# 2. Подготовка данных\n",
    "#################################\n",
    "logging.info(\"Сортировка данных...\")\n",
    "df = df.sort_values(by=['serial_number', 'date'])\n",
    "\n",
    "logging.info(\"Формирование целевой метки (failure_future_30d)...\")\n",
    "\n",
    "def assign_future_failure(group):\n",
    "    group = group.sort_values('date')\n",
    "    failure_dates = pd.to_datetime(group.loc[group['failure'] == 1, 'date'].values)\n",
    "    future_fail_list = []\n",
    "    for _, row in group.iterrows():\n",
    "        current_date = row['date']\n",
    "        idx = np.searchsorted(failure_dates, current_date)\n",
    "        if idx < len(failure_dates) and (failure_dates[idx] <= current_date + pd.Timedelta(days=30)):\n",
    "            future_fail_list.append(1)\n",
    "        else:\n",
    "            future_fail_list.append(0)\n",
    "    group['failure_future_30d'] = future_fail_list\n",
    "    return group\n",
    "\n",
    "serial_groups = df.groupby('serial_number', group_keys=False)\n",
    "df = pd.concat([assign_future_failure(g) for _, g in tqdm(serial_groups, desc='Обработка дисков')])\n",
    "\n",
    "logging.info(\"Удаление записей без будущего периода 30 дней...\")\n",
    "def filter_last_30_days(g):\n",
    "    max_date = g['date'].max()\n",
    "    cutoff = max_date - timedelta(days=30)\n",
    "    return g[g['date'] <= cutoff]\n",
    "\n",
    "df = df.groupby('serial_number', group_keys=False).apply(filter_last_30_days)\n",
    "\n",
    "#################################\n",
    "# 3. Генерация признаков\n",
    "#################################\n",
    "logging.info(\"Генерация признаков...\")\n",
    "smart_cols = [c for c in df.columns if 'smart_' in c and 'raw' in c]\n",
    "\n",
    "def generate_features(g, window=7):\n",
    "    g = g.sort_values('date')\n",
    "    for col in smart_cols:\n",
    "        g[col + '_diff'] = g[col].diff().fillna(0)\n",
    "        g[col + '_ma'] = g[col].rolling(window, min_periods=1).mean().bfill()\n",
    "        g[col + '_diff_ma'] = g[col + '_diff'].rolling(window, min_periods=1).mean().bfill()\n",
    "    return g\n",
    "\n",
    "serial_groups = df.groupby('serial_number', group_keys=False)\n",
    "df = pd.concat([generate_features(g) for _, g in tqdm(serial_groups, desc='Генерация признаков')])\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "#################################\n",
    "# 4. Формирование выборок\n",
    "#################################\n",
    "logging.info(\"Подготовка данных для обучения...\")\n",
    "\n",
    "# Формируем список признаков\n",
    "feature_cols = []\n",
    "for col in smart_cols:\n",
    "    feature_cols.extend([col, col+'_diff', col+'_ma', col+'_diff_ma'])\n",
    "feature_cols.append('capacity_bytes')\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['failure_future_30d']\n",
    "\n",
    "# Разделение по времени\n",
    "logging.info(\"Разделение на train/test по времени...\")\n",
    "min_date = df['date'].min()\n",
    "max_date = df['date'].max()\n",
    "train_end_date = min_date + (max_date - min_date)*0.8\n",
    "\n",
    "train_data = df[df['date'] <= train_end_date]\n",
    "test_data = df[df['date'] > train_end_date]\n",
    "\n",
    "X_train = train_data[feature_cols]\n",
    "y_train = train_data['failure_future_30d']\n",
    "X_test = test_data[feature_cols]\n",
    "y_test = test_data['failure_future_30d']\n",
    "\n",
    "#################################\n",
    "# 5. Первоначальное обучение для оценки важности признаков\n",
    "#################################\n",
    "logging.info(\"Первичное обучение для оценки важности признаков...\")\n",
    "model_initial = LGBMClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "model_initial.fit(X_train, y_train)\n",
    "\n",
    "feature_importances = model_initial.feature_importances_\n",
    "feature_names = np.array(feature_cols)\n",
    "\n",
    "top_n = 30\n",
    "important_features_idx = np.argsort(feature_importances)[-top_n:]\n",
    "important_features = feature_names[important_features_idx]\n",
    "\n",
    "X_train_important = X_train[important_features]\n",
    "X_test_important = X_test[important_features]\n",
    "\n",
    "#################################\n",
    "# 6. Балансировка классов SMOTE\n",
    "#################################\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "logging.info(f\"Дисбаланс классов: {neg_count} отриц. vs {pos_count} полож., scale_pos_weight={scale_pos_weight}\")\n",
    "\n",
    "logging.info(\"Применение SMOTE для балансировки...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train_important, y_train)\n",
    "\n",
    "#################################\n",
    "# 7. Обучение модели с отобранными признаками и scale_pos_weight\n",
    "#################################\n",
    "logging.info(\"Обучение финальной модели LightGBM...\")\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "model.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "#################################\n",
    "# 8. Предсказание и подбор порога\n",
    "#################################\n",
    "logging.info(\"Предсказание вероятностей на тестовой выборке...\")\n",
    "y_proba = model.predict_proba(X_test_important)[:, 1]\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "logging.info(f\"Оптимальный порог для максимизации F1-score: {best_threshold}\")\n",
    "\n",
    "y_pred_adjusted = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "#################################\n",
    "# 9. Оценка результатов\n",
    "#################################\n",
    "logging.info(\"Оценка качества...\")\n",
    "print(\"Classification Report (с отбором признаков и новым порогом):\")\n",
    "print(classification_report(y_test, y_pred_adjusted))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_adjusted)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Визуализация распределения вероятностей\n",
    "plt.figure()\n",
    "plt.hist(y_proba[y_test==1], bins=50, alpha=0.5, label='Positives')\n",
    "plt.hist(y_proba[y_test==0], bins=50, alpha=0.5, label='Negatives')\n",
    "plt.title(\"Distribution of predicted probabilities\")\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "logging.info(\"Готово!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c7c51-2da0-4258-9ce0-cc33b0b3716f",
   "metadata": {},
   "source": [
    "Модель стала предсказывать часть позитивных случаев (Recall для класса 1 теперь ~53%), но качество всё ещё неудовлетворительно: Precision для позитивного класса практически нулевой. Это означает, что модель распознаёт некоторые реальные отказы, но делает это ценой огромного числа ложных тревог.\n",
    "\n",
    "Причина в том, что при выбранном пороге (threshold=1.0) модель начинает маркировать очень много объектов как «1», при этом большая их часть оказывается ложными срабатываниями. Порог в 1.0 означает, что модель очень редко присваивает вероятность строго равную 1.0. Вероятно, это говорит о том, что при расчёте порога по F1-score модель оказалась в таком режиме, что небольшие неточности вычисления или редкие случаи высокой вероятности исказили подбор порога."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f50427ed-17ba-4d49-aff5-63fa7a3a3ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 19:02:37,135 INFO: Проверка версии LightGBM...\n",
      "2024-12-08 19:02:37,138 INFO: Текущая версия LightGBM: 4.5.0\n",
      "2024-12-08 19:02:37,139 INFO: Начало загрузки данных...\n",
      "2024-12-08 19:02:42,134 INFO: Данные загружены. Количество строк: 7320142, Количество колонок: 16\n",
      "2024-12-08 19:02:42,134 INFO: Начало предобработки данных...\n",
      "2024-12-08 19:02:42,445 INFO: Количество пропусков в каждой колонке:\n",
      "2024-12-08 19:02:42,446 INFO: \n",
      "date              0\n",
      "serial_number     0\n",
      "model             0\n",
      "capacity_bytes    0\n",
      "failure           0\n",
      "smart_5_raw       0\n",
      "smart_9_raw       0\n",
      "smart_187_raw     0\n",
      "smart_188_raw     0\n",
      "smart_192_raw     0\n",
      "smart_197_raw     0\n",
      "smart_198_raw     0\n",
      "smart_199_raw     0\n",
      "smart_240_raw     0\n",
      "smart_241_raw     0\n",
      "smart_242_raw     0\n",
      "dtype: int64\n",
      "2024-12-08 19:02:42,554 INFO: Заполнение пропусков средними значениями для числовых признаков...\n",
      "2024-12-08 19:02:43,352 INFO: Пропуски заполнены.\n",
      "2024-12-08 19:02:43,352 INFO: Кодирование категориального признака 'model'...\n",
      "2024-12-08 19:02:43,868 INFO: Кодирование завершено.\n",
      "2024-12-08 19:02:43,868 INFO: Начало инженерии признаков...\n",
      "2024-12-08 19:02:44,602 INFO: Данные отсортированы по 'serial_number' и 'date'.\n",
      "2024-12-08 19:02:44,602 INFO: Создание агрегированных признаков с использованием скользящего окна...\n",
      "Агрегирование признаков: 100%|██████████████████| 11/11 [00:24<00:00,  2.21s/it]\n",
      "2024-12-08 19:03:08,876 INFO: Агрегированные признаки созданы.\n",
      "2024-12-08 19:03:08,877 INFO: Создание целевого признака 'failure_future'...\n",
      "2024-12-08 19:03:09,179 INFO: Целевой признак создан.\n",
      "2024-12-08 19:03:09,185 INFO: Удаление записей, у которых 'date' > max_date - 30 дней...\n",
      "2024-12-08 19:03:09,962 INFO: Удалено 322370 записей. Оставшиеся записи: 6997772\n",
      "2024-12-08 19:03:09,962 INFO: Выбор признаков для модели...\n",
      "2024-12-08 19:03:09,963 INFO: Выбранные признаки: ['capacity_bytes', 'smart_5_raw', 'smart_9_raw', 'smart_187_raw', 'smart_188_raw', 'smart_192_raw', 'smart_197_raw', 'smart_198_raw', 'smart_199_raw', 'smart_240_raw', 'smart_241_raw', 'smart_242_raw', 'model_encoded', 'smart_5_raw_mean_7', 'smart_5_raw_std_7', 'smart_9_raw_mean_7', 'smart_9_raw_std_7', 'smart_187_raw_mean_7', 'smart_187_raw_std_7', 'smart_188_raw_mean_7', 'smart_188_raw_std_7', 'smart_192_raw_mean_7', 'smart_192_raw_std_7', 'smart_197_raw_mean_7', 'smart_197_raw_std_7', 'smart_198_raw_mean_7', 'smart_198_raw_std_7', 'smart_199_raw_mean_7', 'smart_199_raw_std_7', 'smart_240_raw_mean_7', 'smart_240_raw_std_7', 'smart_241_raw_mean_7', 'smart_241_raw_std_7', 'smart_242_raw_mean_7', 'smart_242_raw_std_7']\n",
      "2024-12-08 19:03:09,963 INFO: Целевая переменная: failure_future\n",
      "2024-12-08 19:03:09,963 INFO: Разделение данных на обучающую и тестовую выборки...\n",
      "2024-12-08 19:03:11,380 INFO: Обучающая выборка: (5598217, 40), Тестовая выборка: (1399555, 40)\n",
      "2024-12-08 19:03:11,738 INFO: Распределение классов в обучающей выборке:\n",
      "failure_future\n",
      "0    5598086\n",
      "1        131\n",
      "Name: count, dtype: int64\n",
      "2024-12-08 19:03:11,738 INFO: Проверка наличия пропусков в обучающей и тестовой выборках...\n",
      "2024-12-08 19:03:11,889 INFO: Количество пропусков в обучающей выборке: 119306\n",
      "2024-12-08 19:03:11,890 INFO: Количество пропусков в тестовой выборке: 737\n",
      "2024-12-08 19:03:11,890 INFO: Заполнение оставшихся пропусков с помощью SimpleImputer...\n",
      "2024-12-08 19:03:30,736 INFO: Пропуски заполнены.\n",
      "2024-12-08 19:03:30,738 INFO: Обработка дисбаланса классов с помощью SMOTE...\n",
      "2024-12-08 19:03:38,640 INFO: После применения SMOTE - распределение классов:\n",
      "failure_future\n",
      "0    5598086\n",
      "1    5598086\n",
      "Name: count, dtype: int64\n",
      "2024-12-08 19:03:38,642 INFO: Начало обучения модели LightGBM (LGBMClassifier)...\n",
      "2024-12-08 19:03:39,598 ERROR: Произошла ошибка во время выполнения скрипта.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/x4/2szdcyyx23qg3vs2t16zmy1m0000gn/T/ipykernel_83784/988505461.py\", line 190, in main\n",
      "    lgbm.fit(\n",
      "TypeError: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from packaging import version  # Добавлен импорт для сравнения версий\n",
    "\n",
    "# Настройка логгирования\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"smart_failure_prediction.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Проверка версии LightGBM\n",
    "        logger.info(\"Проверка версии LightGBM...\")\n",
    "        import lightgbm as lgb\n",
    "        lgb_version = lgb.__version__\n",
    "        logger.info(f\"Текущая версия LightGBM: {lgb_version}\")\n",
    "        \n",
    "        # Рекомендуемая минимальная версия LightGBM для поддержки 'early_stopping_rounds'\n",
    "        min_required_version = '3.0.0'\n",
    "        if version.parse(lgb_version) < version.parse(min_required_version):\n",
    "            logger.error(f\"Установленная версия LightGBM ({lgb_version}) ниже требуемой ({min_required_version}). Пожалуйста, обновите LightGBM.\")\n",
    "            logger.info(\"Обновление LightGBM...\")\n",
    "            import subprocess\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"lightgbm\"])\n",
    "            logger.info(\"LightGBM успешно обновлён. Пожалуйста, перезапустите скрипт.\")\n",
    "            return  # Завершаем выполнение скрипта после обновления\n",
    "\n",
    "        # 1. Загрузка данных\n",
    "        logger.info(\"Начало загрузки данных...\")\n",
    "        data = pd.read_csv('ST14000NM001G.csv', parse_dates=['date'])\n",
    "        logger.info(f\"Данные загружены. Количество строк: {data.shape[0]}, Количество колонок: {data.shape[1]}\")\n",
    "\n",
    "        # 2. Предобработка данных\n",
    "        logger.info(\"Начало предобработки данных...\")\n",
    "        # Проверка на пропуски\n",
    "        missing = data.isnull().sum()\n",
    "        logger.info(\"Количество пропусков в каждой колонке:\")\n",
    "        logger.info(f\"\\n{missing}\")\n",
    "\n",
    "        # Заполнение пропусков средним значением для числовых признаков\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        logger.info(\"Заполнение пропусков средними значениями для числовых признаков...\")\n",
    "        data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
    "        logger.info(\"Пропуски заполнены.\")\n",
    "\n",
    "        # Кодирование категориальных признаков\n",
    "        if 'model' in data.columns:\n",
    "            logger.info(\"Кодирование категориального признака 'model'...\")\n",
    "            label_encoder = LabelEncoder()\n",
    "            data['model_encoded'] = label_encoder.fit_transform(data['model'])\n",
    "            logger.info(\"Кодирование завершено.\")\n",
    "        else:\n",
    "            logger.warning(\"Категориальный признак 'model' отсутствует в данных.\")\n",
    "\n",
    "        # 3. Инженерия признаков\n",
    "        logger.info(\"Начало инженерии признаков...\")\n",
    "        data = data.sort_values(['serial_number', 'date'])\n",
    "        logger.info(\"Данные отсортированы по 'serial_number' и 'date'.\")\n",
    "\n",
    "        # Определение SMART-признаков\n",
    "        smart_features = ['smart_5_raw', 'smart_9_raw', 'smart_187_raw',\n",
    "                         'smart_188_raw', 'smart_192_raw', 'smart_197_raw',\n",
    "                         'smart_198_raw', 'smart_199_raw', 'smart_240_raw',\n",
    "                         'smart_241_raw', 'smart_242_raw']\n",
    "\n",
    "        window = 7  # последние 7 дней\n",
    "\n",
    "        # Используем tqdm для отображения прогресса при создании агрегированных признаков\n",
    "        logger.info(\"Создание агрегированных признаков с использованием скользящего окна...\")\n",
    "        for feature in tqdm(smart_features, desc=\"Агрегирование признаков\"):\n",
    "            mean_col = f'{feature}_mean_{window}'\n",
    "            std_col = f'{feature}_std_{window}'\n",
    "            data[mean_col] = data.groupby('serial_number')[feature].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "            data[std_col] = data.groupby('serial_number')[feature].transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "        logger.info(\"Агрегированные признаки созданы.\")\n",
    "\n",
    "        # Создание целевого признака\n",
    "        logger.info(\"Создание целевого признака 'failure_future'...\")\n",
    "        data['failure_future'] = data.groupby('serial_number')['failure'].shift(-30)\n",
    "        data['failure_future'] = data['failure_future'].fillna(0).astype(int)\n",
    "        logger.info(\"Целевой признак создан.\")\n",
    "\n",
    "        # Удаление записей без целевого значения\n",
    "        max_date = data['date'].max()\n",
    "        logger.info(\"Удаление записей, у которых 'date' > max_date - 30 дней...\")\n",
    "        initial_shape = data.shape\n",
    "        data = data[data['date'] <= (max_date - timedelta(days=30))]\n",
    "        logger.info(f\"Удалено {initial_shape[0] - data.shape[0]} записей. Оставшиеся записи: {data.shape[0]}\")\n",
    "\n",
    "        # 4. Выбор признаков для модели\n",
    "        logger.info(\"Выбор признаков для модели...\")\n",
    "        feature_cols = [\n",
    "            'capacity_bytes',\n",
    "            'smart_5_raw', 'smart_9_raw', 'smart_187_raw',\n",
    "            'smart_188_raw', 'smart_192_raw', 'smart_197_raw',\n",
    "            'smart_198_raw', 'smart_199_raw', 'smart_240_raw',\n",
    "            'smart_241_raw', 'smart_242_raw',\n",
    "            'model_encoded'\n",
    "        ]\n",
    "\n",
    "        for feature in smart_features:\n",
    "            feature_mean = f'{feature}_mean_{window}'\n",
    "            feature_std = f'{feature}_std_{window}'\n",
    "            feature_cols.extend([feature_mean, feature_std])\n",
    "\n",
    "        target = 'failure_future'\n",
    "        logger.info(f\"Выбранные признаки: {feature_cols}\")\n",
    "        logger.info(f\"Целевая переменная: {target}\")\n",
    "\n",
    "        # 5. Разделение данных на обучающую и тестовую выборки\n",
    "        logger.info(\"Разделение данных на обучающую и тестовую выборки...\")\n",
    "        # Сортируем данные по дате\n",
    "        data = data.sort_values('date')\n",
    "        # Определяем индекс для разделения (например, 80% для обучения, 20% для теста)\n",
    "        split_ratio = 0.8\n",
    "        split_index = int(len(data) * split_ratio)\n",
    "        train_data = data.iloc[:split_index]\n",
    "        test_data = data.iloc[split_index:]\n",
    "        logger.info(f\"Обучающая выборка: {train_data.shape}, Тестовая выборка: {test_data.shape}\")\n",
    "\n",
    "        # Проверяем, не пуста ли тестовая выборка\n",
    "        if test_data.empty:\n",
    "            logger.error(\"Тестовая выборка пуста после разделения. Проверьте условия разделения данных.\")\n",
    "            return\n",
    "\n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data[target]\n",
    "        X_test = test_data[feature_cols]\n",
    "        y_test = test_data[target]\n",
    "        logger.info(f\"Распределение классов в обучающей выборке:\\n{y_train.value_counts()}\")\n",
    "\n",
    "        # 6. Обработка оставшихся пропусков перед SMOTE\n",
    "        logger.info(\"Проверка наличия пропусков в обучающей и тестовой выборках...\")\n",
    "        train_missing = X_train.isnull().sum().sum()\n",
    "        test_missing = X_test.isnull().sum().sum()\n",
    "        logger.info(f\"Количество пропусков в обучающей выборке: {train_missing}\")\n",
    "        logger.info(f\"Количество пропусков в тестовой выборке: {test_missing}\")\n",
    "\n",
    "        if train_missing > 0 or test_missing > 0:\n",
    "            logger.info(\"Заполнение оставшихся пропусков с помощью SimpleImputer...\")\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "            logger.info(\"Пропуски заполнены.\")\n",
    "        else:\n",
    "            logger.info(\"Пропусков не обнаружено.\")\n",
    "\n",
    "        # 7. Обработка дисбаланса классов с помощью SMOTE\n",
    "        logger.info(\"Обработка дисбаланса классов с помощью SMOTE...\")\n",
    "        # Исправление предупреждения о параметре `n_jobs`:\n",
    "        # Удаляем `n_jobs=-1` и используем ближайших соседей с установленным `n_jobs`.\n",
    "        smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        logger.info(f\"После применения SMOTE - распределение классов:\\n{pd.Series(y_train_res).value_counts()}\")\n",
    "\n",
    "        # 8. Обучение модели LightGBM с использованием LGBMClassifier\n",
    "        logger.info(\"Начало обучения модели LightGBM (LGBMClassifier)...\")\n",
    "        lgbm = LGBMClassifier(\n",
    "            objective='binary',\n",
    "            metric='auc',\n",
    "            boosting_type='gbdt',\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            max_depth=-1,\n",
    "            verbose=-1,\n",
    "            random_state=42,\n",
    "            scale_pos_weight=(len(y_train_res) - sum(y_train_res)) / sum(y_train_res),\n",
    "            n_estimators=1000\n",
    "        )\n",
    "\n",
    "        # Используем раннюю остановку\n",
    "        lgbm.fit(\n",
    "            X_train_res, y_train_res,\n",
    "            eval_set=[(X_train_res, y_train_res), (X_test, y_test)],\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=100\n",
    "        )\n",
    "        logger.info(\"Обучение модели завершено.\")\n",
    "\n",
    "        # 9. Оценка модели\n",
    "        logger.info(\"Начало оценки модели...\")\n",
    "        y_pred_proba = lgbm.predict_proba(X_test)[:,1]\n",
    "        y_pred = lgbm.predict(X_test)\n",
    "\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        logger.info(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        logger.info(\"Отчет по классификации:\")\n",
    "        clf_report = classification_report(y_test, y_pred)\n",
    "        logger.info(f\"\\n{clf_report}\")\n",
    "\n",
    "        logger.info(\"Матрица ошибок:\")\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Предсказанные классы')\n",
    "        plt.ylabel('Истинные классы')\n",
    "        plt.title('Матрица ошибок')\n",
    "        plt.show()\n",
    "        logger.info(\"Матрица ошибок отображена.\")\n",
    "\n",
    "        # 10. Важность признаков\n",
    "        logger.info(\"Анализ важности признаков...\")\n",
    "        feature_importances = pd.DataFrame({\n",
    "            'feature': lgbm.feature_name_,\n",
    "            'importance': lgbm.feature_importances_\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importances.head(20))\n",
    "        plt.title('Важность признаков')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        logger.info(\"Важность признаков отображена.\")\n",
    "\n",
    "        # 11. Прогнозирование отказов на тестовой выборке\n",
    "        logger.info(\"Создание предсказаний на тестовой выборке...\")\n",
    "        test_data = test_data.copy()\n",
    "        test_data['failure_pred'] = y_pred\n",
    "        test_data['failure_proba'] = y_pred_proba\n",
    "\n",
    "        # Сохранение результатов\n",
    "        logger.info(\"Сохранение предсказаний в 'smart_failure_predictions.csv'...\")\n",
    "        test_data.to_csv('smart_failure_predictions.csv', index=False)\n",
    "        logger.info(\"Предсказания сохранены успешно.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Произошла ошибка во время выполнения скрипта.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac218250-f8f1-473d-a4fa-77cdd910f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/e8/30/35111dae435c640694d616a611b7ff6b2482cfd977f8f572ff960a321d66/optuna-4.1.0-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from optuna) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from optuna) (23.2)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.36-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sqlalchemy, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 colorlog-6.9.0 optuna-4.1.0 sqlalchemy-2.0.36\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d39145-33a1-4322-b451-99e6ea51fd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2471c2a-2a4e-488a-ad2a-0b3fd467f396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e7651a-6942-4a45-9971-30b564ba2d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
